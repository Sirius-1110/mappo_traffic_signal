{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\Eddie\\Documents\\marl_sigctrl\\framework\")\n",
    "sys.path.append(r\"C:\\Users\\Eddie\\Documents\\marl_sigctrl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from collections import namedtuple\n",
    "from ding.rl_utils import ppo_policy_error\n",
    "\n",
    "\n",
    "def test_ppo_policy_error():\n",
    "    # 设置测试数据\n",
    "    logit_new = torch.tensor([[0.1, 0.2, 0.7], [0.6, 0.2, 0.2]])\n",
    "    logit_old = torch.tensor([[0.1, 0.2, 0.7], [0.6, 0.2, 0.2]])\n",
    "    action = torch.tensor([2, 0])\n",
    "    adv = torch.tensor([0.5, -0.5])\n",
    "    weight = torch.tensor([1.0, 1.0])\n",
    "    clip_ratio = 0.2\n",
    "    dual_clip = None\n",
    "\n",
    "    data = namedtuple('Data', ['logit_new', 'logit_old', 'action', 'adv', 'weight'])\n",
    "    data = data(logit_new, logit_old, action, adv, weight)\n",
    "\n",
    "    # 调用被测方法\n",
    "    loss, info = ppo_policy_error(data, clip_ratio, dual_clip)\n",
    "\n",
    "    # 验证损失和信息\n",
    "    assert loss.policy_loss.item() >= 0\n",
    "    assert loss.entropy_loss.item() >= 0\n",
    "    assert info.approx_kl >= 0\n",
    "    assert 0 <= info.clipfrac <= 1\n",
    "\n",
    "\n",
    "def test_ppo_policy_error_DualClip():\n",
    "    # 设置测试数据\n",
    "    logit_new = torch.tensor([[0.1, 0.2, 0.7], [0.6, 0.2, 0.2]])\n",
    "    logit_old = torch.tensor([[0.1, 0.2, 0.7], [0.6, 0.2, 0.2]])\n",
    "    action = torch.tensor([2, 0])\n",
    "    adv = torch.tensor([0.5, -0.5])\n",
    "    weight = torch.tensor([1.0, 1.0])\n",
    "    clip_ratio = 0.2\n",
    "    dual_clip = 0.5\n",
    "\n",
    "    data = namedtuple('Data', ['logit_new', 'logit_old', 'action', 'adv', 'weight'])\n",
    "    data = data(logit_new, logit_old, action, adv, weight)\n",
    "\n",
    "    # 调用被测方法\n",
    "    loss, info = ppo_policy_error(data, clip_ratio, dual_clip)\n",
    "\n",
    "    # 验证损失和信息\n",
    "    assert loss.policy_loss.item() >= 0\n",
    "    assert loss.entropy_loss.item() >= 0\n",
    "    assert info.approx_kl >= 0\n",
    "    assert 0 <= info.clipfrac <= 1\n",
    "\n",
    "\n",
    "def test_ppo_policy_error_NoWeight():\n",
    "    # 设置测试数据\n",
    "    logit_new = torch.tensor([[0.1, 0.2, 0.7], [0.6, 0.2, 0.2]])\n",
    "    logit_old = torch.tensor([[0.1, 0.2, 0.7], [0.6, 0.2, 0.2]])\n",
    "    action = torch.tensor([2, 0])\n",
    "    adv = torch.tensor([0.5, -0.5])\n",
    "    weight = None\n",
    "    clip_ratio = 0.2\n",
    "    dual_clip = None\n",
    "\n",
    "    data = namedtuple('Data', ['logit_new', 'logit_old', 'action', 'adv', 'weight'])\n",
    "    data = data(logit_new, logit_old, action, adv, weight)\n",
    "\n",
    "    # 调用被测方法\n",
    "    loss, info = ppo_policy_error(data, clip_ratio, dual_clip)\n",
    "\n",
    "    # 验证损失和信息\n",
    "    assert loss.policy_loss.item() >= 0\n",
    "    assert loss.entropy_loss.item() >= 0\n",
    "    assert info.approx_kl >= 0\n",
    "    assert 0 <= info.clipfrac <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_ppo_policy_error_DualClip()\n",
      "Cell \u001b[1;32mIn[4], line 47\u001b[0m, in \u001b[0;36mtest_ppo_policy_error_DualClip\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m loss, info \u001b[38;5;241m=\u001b[39m ppo_policy_error(data, clip_ratio, dual_clip)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 验证损失和信息\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mpolicy_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mentropy_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m info\u001b[38;5;241m.\u001b[39mapprox_kl \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "test_ppo_policy_error_DualClip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "被测方法行为:\n",
    " gae 函数实现了广义优势估计器（Generalized Advantage Estimator，GAE），这是一种用于计算强化学习中优势估计的算法。该算法使用折扣因子 gamma 和 GAE 参数 lambda_ 来计算优势。函数接收一个包含价值、下一个价值、奖励、完成标志和轨迹标志的命名元组。它处理这些输入，计算每个时间步的优势，并返回一个优势张量。\n",
    "\n",
    "分支和所需的测试用例:\n",
    "\n",
    "- [1]**分支 1**: 如果 done 为 None，则将其初始化为与 reward 形状相同的零张量。\n",
    "- [2]**分支 2**: 如果 traj_flag 为 None，则将其设置为 done。\n",
    "- [3]**分支 3**: 如果 value 的维度比 reward 多一个，则对 reward、done 和 traj_flag 进行扩展。\n",
    "- [4]**循环**: 从最后一个时间步向前迭代，计算 GAE。\n",
    "\n",
    "模拟需求: 不需要模拟，因为该函数直接处理输入张量，没有外部依赖或需要模拟的复杂交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个智能体对应一个优势函数估计值：当 value 的形状为 (T, B) 时，每个智能体有一个优势估计值。\n",
    "# 每个智能体的每一个动作都有一个估计值：当 value 的形状为 (T, B, A) 时，每个智能体的每个动作都有一个优势估计值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ding.rl_utils import gae\n",
    "def test_gae():\n",
    "    # 设置测试数据\n",
    "    T, B = 5, 3\n",
    "    value = torch.randn(T, B)\n",
    "    next_value = torch.randn(T, B)\n",
    "    reward = torch.randn(T, B)\n",
    "    done = torch.randint(0, 2, (T, B)).float()\n",
    "    traj_flag = torch.randint(0, 2, (T, B)).float()\n",
    "    data = namedtuple('Data', ['value', 'next_value', 'reward', 'done', 'traj_flag'])(value, next_value, reward, done, traj_flag)\n",
    "\n",
    "    # 使用默认参数调用 GAE\n",
    "    adv = gae(data)\n",
    "    print(adv)\n",
    "    # 验证输出形状\n",
    "    assert adv.shape == (T, B), \"Advantage tensor shape is incorrect.\"\n",
    "\n",
    "    # 验证计算\n",
    "    gamma = 0.99\n",
    "    lambda_ = 0.97\n",
    "    next_value *= (1 - done)\n",
    "    delta = reward + gamma * next_value - value\n",
    "    factor = gamma * lambda_ * (1 - traj_flag)\n",
    "    expected_adv = torch.zeros_like(value)\n",
    "    gae_item = torch.zeros_like(value[0])\n",
    "    for t in reversed(range(T)):\n",
    "        gae_item = delta[t] + factor[t] * gae_item\n",
    "        expected_adv[t] = gae_item\n",
    "\n",
    "    assert torch.allclose(adv, expected_adv), \"Calculated advantage does not match expected values.\"\n",
    "\n",
    "    # 测试当 done 和 traj_flag 为 None 时\n",
    "    data_none_flags = namedtuple('Data', ['value', 'next_value', 'reward', 'done', 'traj_flag'])(value, next_value, reward, None, None)\n",
    "    adv_none_flags = gae(data_none_flags)\n",
    "    assert torch.allclose(adv_none_flags, expected_adv), \"Advantage calculation with None done/traj_flag is incorrect.\"\n",
    "\n",
    "    # 测试当 value 的维度比 reward 多一个时\n",
    "    value_expanded = torch.randn(T, B, 1)\n",
    "    data_expanded = namedtuple('Data', ['value', 'next_value', 'reward', 'done', 'traj_flag'])(value_expanded, next_value, reward, done, traj_flag)\n",
    "    adv_expanded = gae(data_expanded)\n",
    "    assert adv_expanded.shape == (T, B, 1), \"Advantage tensor shape is incorrect for expanded value.\"\n",
    "    assert torch.allclose(adv_expanded.squeeze(-1), expected_adv), \"Calculated advantage for expanded value does not match expected values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8593,  0.4486,  3.6517],\n",
      "        [ 3.0480, -0.4445,  3.6012],\n",
      "        [-0.8671,  1.9734,  1.0996],\n",
      "        [ 0.8457, -1.2078,  0.2137],\n",
      "        [ 1.7281,  0.2591,  0.3168]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Advantage calculation with None done/traj_flag is incorrect.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m test_gae()\n",
      "\n",
      "Cell \u001b[1;32mIn[9], line 35\u001b[0m, in \u001b[0;36mtest_gae\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     33\u001b[0m data_none_flags \u001b[38;5;241m=\u001b[39m namedtuple(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraj_flag\u001b[39m\u001b[38;5;124m'\u001b[39m])(value, next_value, reward, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;32m     34\u001b[0m adv_none_flags \u001b[38;5;241m=\u001b[39m gae(data_none_flags)\n",
      "\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(adv_none_flags, expected_adv), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdvantage calculation with None done/traj_flag is incorrect.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 测试当 value 的维度比 reward 多一个时\u001b[39;00m\n",
      "\u001b[0;32m     38\u001b[0m value_expanded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(T, B, \u001b[38;5;241m1\u001b[39m)\n",
      "\n",
      "\u001b[1;31mAssertionError\u001b[0m: Advantage calculation with None done/traj_flag is incorrect."
     ]
    }
   ],
   "source": [
    "test_gae()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# torch.no_grad() 的作用\n",
    "在 PyTorch 中，torch.no_grad() 是一个上下文管理器，用于在上下文环境中关闭梯度计算。\n",
    "\n",
    "在深度学习训练过程中，PyTorch 会自动跟踪所有涉及到 torch.Tensor 的操作，以便后续进行反向传播计算梯度。\n",
    "\n",
    "然而，在某些情况下，我们并不需要计算梯度，例如在模型推理阶段或者进行一些不需要更新参数的计算时，关闭梯度计算可以带来以下好处：\n",
    "\n",
    "- 节省内存：由于不需要存储中间计算结果用于反向传播，因此可以减少内存的使用。\n",
    "- 提高计算速度：避免了梯度计算的开销，从而加快了计算速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import unittest\n",
    "\n",
    "class TestNoGrad(unittest.TestCase):\n",
    "    def test_no_grad(self):\n",
    "        # 定义一个简单的线性模型\n",
    "        model = torch.nn.Linear(10, 1)\n",
    "        # 生成一个输入张量\n",
    "        x = torch.randn(1, 10)\n",
    "\n",
    "        # 在有梯度计算的情况下进行前向传播\n",
    "        with torch.enable_grad():\n",
    "            y_with_grad = model(x)\n",
    "            self.assertEqual(y_with_grad.requires_grad, False, \"梯度计算应该是开启的\")\n",
    "\n",
    "        # 在无梯度计算的情况下进行前向传播\n",
    "        with torch.no_grad():\n",
    "            y_no_grad = model(x)\n",
    "            self.assertEqual(y_no_grad.requires_grad, False, \"梯度计算应该是关闭的\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "True != False : 梯度计算应该是开启的",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m no_grad_test \u001b[38;5;241m=\u001b[39m TestNoGrad()\n\u001b[1;32m----> 2\u001b[0m no_grad_test\u001b[38;5;241m.\u001b[39mtest_no_grad()\n",
      "Cell \u001b[1;32mIn[19], line 14\u001b[0m, in \u001b[0;36mTestNoGrad.test_no_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m     13\u001b[0m     y_with_grad \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massertEqual(y_with_grad\u001b[38;5;241m.\u001b[39mrequires_grad, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m梯度计算应该是开启的\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 在无梯度计算的情况下进行前向传播\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\unittest\\case.py:873\u001b[0m, in \u001b[0;36mTestCase.assertEqual\u001b[1;34m(self, first, second, msg)\u001b[0m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fail if the two objects are unequal as determined by the '=='\u001b[39;00m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m   operator.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    872\u001b[0m assertion_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getAssertEqualityFunc(first, second)\n\u001b[1;32m--> 873\u001b[0m assertion_func(first, second, msg\u001b[38;5;241m=\u001b[39mmsg)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\unittest\\case.py:866\u001b[0m, in \u001b[0;36mTestCase._baseAssertEqual\u001b[1;34m(self, first, second, msg)\u001b[0m\n\u001b[0;32m    864\u001b[0m standardMsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m _common_shorten_repr(first, second)\n\u001b[0;32m    865\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_formatMessage(msg, standardMsg)\n\u001b[1;32m--> 866\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfailureException(msg)\n",
      "\u001b[1;31mAssertionError\u001b[0m: True != False : 梯度计算应该是开启的"
     ]
    }
   ],
   "source": [
    "no_grad_test = TestNoGrad()\n",
    "no_grad_test.test_no_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义Critic网络来估计Q值和V值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全局状态: tensor([[-0.2505,  0.2877,  1.8954,  0.6721,  1.6329,  0.7981, -1.7562,  1.1579,\n",
      "         -0.0837, -2.8926,  0.5453,  0.4943]])\n",
      "联合动作: tensor([[1., 0., 1.]])\n",
      "优势函数估计值: 0.021983802318572998\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        # 用于估计Q值的网络\n",
    "        q_layers = []\n",
    "\n",
    "        # 状态价值对-优势估计\n",
    "        input_dim = state_dim + action_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            q_layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            q_layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "        q_layers.append(nn.Linear(input_dim, 1))\n",
    "        self.q_network = nn.Sequential(*q_layers)\n",
    "\n",
    "        # 用于估计V值的网络\n",
    "        v_layers = []\n",
    "        input_dim = state_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            v_layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            v_layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "        v_layers.append(nn.Linear(input_dim, 1))\n",
    "        self.v_network = nn.Sequential(*v_layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # 计算Q值\n",
    "        q_input = torch.cat([state, action], dim=-1)\n",
    "        q_value = self.q_network(q_input)\n",
    "        # 计算V值\n",
    "        v_value = self.v_network(state)\n",
    "        # 计算优势函数值\n",
    "        advantage = q_value - v_value\n",
    "        return advantage\n",
    "\n",
    "# 单元测试\n",
    "def test_critic_network():\n",
    "    # 定义参数\n",
    "    num_intersections = 3  # 交叉口数量\n",
    "    state_features_per_intersection = 4  # 每个交叉口的状态特征数量\n",
    "    state_dim = num_intersections * state_features_per_intersection\n",
    "    action_dim = num_intersections\n",
    "    hidden_dims = [16, 8]  # 隐藏层维度\n",
    "\n",
    "    # 创建Critic网络实例\n",
    "    critic = CriticNetwork(state_dim, action_dim, hidden_dims)\n",
    "\n",
    "    # 随机生成一个全局状态和联合动作\n",
    "    s_t = torch.randn(1, state_dim)  # 时间步t的全局状态\n",
    "    a_t = torch.randint(0, 2, (1, action_dim)).float()  # 时间步t的联合动作\n",
    "\n",
    "    # 计算优势函数估计值\n",
    "    advantage = critic(s_t, a_t)\n",
    "\n",
    "    print(f\"全局状态: {s_t}\")\n",
    "    print(f\"联合动作: {a_t}\")\n",
    "    print(f\"优势函数估计值: {advantage.item()}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "test_critic_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class MockEnv:\n",
    "    \"\"\"模拟两个交叉口的环境\"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_agents = 2\n",
    "        self.global_state_dim = 4  # 假设全局状态包含两个交叉口的信息\n",
    "        self.local_obs_dim = 2     # 每个Actor观测到自身交叉口的状态\n",
    "        \n",
    "    def reset(self):\n",
    "        return {'global_state': np.random.randn(self.global_state_dim),\n",
    "                'local_obs': [np.random.randn(self.local_obs_dim) for _ in range(self.n_agents)]}\n",
    "    \n",
    "    def step(self, actions):\n",
    "        # 动作空间A={0,1}，随机生成奖励\n",
    "        rewards = [float(act == 1)*0.5 - 0.1 for act in actions]  # 切换相位获得+0.5，否则-0.1\n",
    "        done = False\n",
    "        return {'global_state': np.random.randn(self.global_state_dim),\n",
    "                'local_obs': [np.random.randn(self.local_obs_dim) for _ in range(self.n_agents)],\n",
    "                'rewards': rewards,\n",
    "                'done': done}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralizedCritic(torch.nn.Module):\n",
    "    \"\"\"中心化Critic网络\"\"\"\n",
    "    def __init__(self, global_state_dim):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(global_state_dim, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 1)  # 输出全局状态价值V(s)\n",
    "        )\n",
    "    \n",
    "    def forward(self, global_state):\n",
    "        return self.net(global_state)\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    \"\"\"分散式Actor网络\"\"\"\n",
    "    def __init__(self, local_obs_dim):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(local_obs_dim, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 2),  # 输出动作概率（A={0,1}）\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, local_obs):\n",
    "        return self.net(local_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 actions [0, 1]\n",
      "step 1 actions [0, 0]\n",
      "step 2 actions [1, 1]\n",
      "batch global_states:  [tensor([ 0.6529,  2.1936,  1.2539, -0.5296]), tensor([-0.4851,  0.1526, -0.1222,  0.2268]), tensor([-0.9882,  0.3741, -1.3519, -0.1919])]\n",
      "----------------------------------------\n",
      "batch rewards:  [[-0.1, 0.4], [-0.1, -0.1], [0.4, 0.4]]\n",
      "----------------------------------------\n",
      "batch local_obs:  [[tensor([-0.3493, -1.3071]), tensor([-0.4891,  2.1346])], [tensor([2.6298, 0.7472]), tensor([2.9983, 0.4752])], [tensor([1.0271, 1.7269]), tensor([-0.8710, -0.1085])]]\n",
      "----------------------------------------\n",
      "batch dones:  [False, False, False]\n",
      "----------------------------------------\n",
      "no grad critc values  [-0.49859232  0.03809349  0.33578724]\n",
      "----------------------------------------\n",
      "agent 0 rewards [-0.1, -0.1, 0.4]\n",
      "智能体0优势函数值 [0.6916323171043396, 0.25790650761127476, 0.06421276330947878]\n",
      "agent 1 rewards [0.4, -0.1, 0.4]\n",
      "智能体1优势函数值 [1.1916323171043397, 0.25790650761127476, 0.06421276330947878]\n",
      "测试通过：优势函数按智能体独立计算\n"
     ]
    }
   ],
   "source": [
    "def test_advantage_calculation():\n",
    "    # 初始化环境与模型\n",
    "    env = MockEnv()\n",
    "    critic = CentralizedCritic(env.global_state_dim)\n",
    "    actors = [Actor(env.local_obs_dim) for _ in range(env.n_agents)]\n",
    "    \n",
    "    # 模拟轨迹收集\n",
    "    batch = {'global_states': [], 'rewards': [], 'local_obs': [], 'dones': []}\n",
    "    obs = env.reset()\n",
    "    for i in range(3):  # 收集3步数据\n",
    "        actions = [np.random.choice([0,1], p=actor(torch.Tensor(obs['local_obs'][i])).detach().numpy()) \n",
    "                  for i, actor in enumerate(actors)]\n",
    "        print(f\"step {i} actions\", actions)\n",
    "        next_obs = env.step(actions)\n",
    "\n",
    "        batch['global_states'].append(torch.Tensor(obs['global_state']))\n",
    "        batch['local_obs'].append([torch.Tensor(o) for o in obs['local_obs']])\n",
    "        batch['rewards'].append(next_obs['rewards'])\n",
    "        batch['dones'].append(next_obs['done'])\n",
    "\n",
    "        obs = next_obs\n",
    "    for key, vals in batch.items():\n",
    "        print(f\"batch {key}: \", vals)\n",
    "        print(\"--\"*20)\n",
    "\n",
    "    # 计算优势函数\n",
    "    with torch.no_grad():\n",
    "        values = critic(torch.stack(batch['global_states'])).squeeze().numpy()\n",
    "        print(\"no grad critc values \", values)\n",
    "        print(\"--\"*20)\n",
    "    \n",
    "    # 优势函数计算逻辑（假设gamma=0.99）\n",
    "    advantages = []\n",
    "    for i in range(env.n_agents):\n",
    "        agent_rewards = [r[i] for r in batch['rewards']]\n",
    "        print(f\"agent {i} rewards\", agent_rewards)\n",
    "        agent_advantages = []\n",
    "        running_advantage = 0\n",
    "        for t in reversed(range(len(agent_rewards))):\n",
    "            running_advantage = agent_rewards[t] + 0.99 * running_advantage * (1 - int(batch['dones'][t]))\n",
    "            delta = running_advantage - values[t]\n",
    "            agent_advantages.insert(0, delta)\n",
    "        advantages.append(agent_advantages)\n",
    "        print(f\"智能体{i}优势函数值\", agent_advantages)\n",
    "    # 验证断言\n",
    "    assert len(advantages) == env.n_agents, \"每个智能体应有独立优势序列\"\n",
    "    assert not np.allclose(advantages[0], advantages[1]), \"不同智能体的优势值应不同（因奖励轨迹不同）\"\n",
    "    print(\"测试通过：优势函数按智能体独立计算\")\n",
    "\n",
    "# 执行测试\n",
    "test_advantage_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
